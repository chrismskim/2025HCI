# -*- coding: utf-8 -*-
"""HCI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WBbQKeiQ1pU1cahfiuWsGG18obBQ4NjS
"""


from IPython.display import display, Javascript
from google.colab.output import eval_js
from base64 import b64decode
import numpy as np
import cv2

def capture_image():
    display(Javascript('''
    async function takePhoto() {
      const div = document.createElement('div');
      const capture = document.createElement('button');
      capture.textContent = ' 사진 찍기';
      div.appendChild(capture);

      const video = document.createElement('video');
      video.style.display = 'block';
      const stream = await navigator.mediaDevices.getUserMedia({video: true});

      document.body.appendChild(div);
      div.appendChild(video);
      video.srcObject = stream;
      await video.play();

      // Resize output to fit video stream
      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

      // Wait for capture button click
      await new Promise((resolve) => capture.onclick = resolve);

      const canvas = document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      canvas.getContext('2d').drawImage(video, 0, 0);
      stream.getTracks().forEach(track => track.stop());
      div.remove();

      return canvas.toDataURL('image/jpeg', 0.8);
    }
    takePhoto();
    '''))

    data = eval_js("takePhoto()")
    binary = b64decode(data.split(',')[1])
    jpg = np.frombuffer(binary, dtype=np.uint8)
    image = cv2.imdecode(jpg, cv2.IMREAD_COLOR)
    return image

import mediapipe as mp
from matplotlib import pyplot as plt

mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1)

# LEFT_EYE_INDEXES = [33, 133, 160, 159, 158, 157, 173, 246]
# MOUTH_INDEXES = [61, 146, 91, 181, 84, 17, 314, 405, 321, 375, 291]
LEFT_EYE_INDEXES = [33, 133, 160, 159, 158, 157, 173, 246]
RIGHT_EYE_INDEXES = [362, 263, 387, 386, 385, 384, 398, 466]
MOUTH_INDEXES = [61, 146, 91, 181, 84, 17, 314, 405, 321, 375, 291]
NOSE_INDEXES = [1, 2, 98, 327, 168, 195]

def get_landmark_coords(landmarks, indexes, image_shape):
    h, w, _ = image_shape
    return np.array([[int(landmarks[i].x * w), int(landmarks[i].y * h)] for i in indexes])

def warp_region(image, src_points, scale=1.5):
    src_points = np.array(src_points, dtype=np.float32)
    center = np.mean(src_points, axis=0)
    translated = src_points - center
    scaled = translated * scale
    dst_points = scaled + center
    M = cv2.getAffineTransform(src_points[:3], dst_points[:3])
    warped = cv2.warpAffine(image, M, (image.shape[1], image.shape[0]))
    mask = np.zeros_like(image)
    cv2.fillConvexPoly(mask, np.int32(dst_points), (255, 255, 255))
    warped_region = cv2.bitwise_and(warped, mask)
    background_removed = cv2.bitwise_and(image, cv2.bitwise_not(mask))
    return cv2.add(warped_region, background_removed)

def cartoon_effect(image):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    edges = cv2.adaptiveThreshold(cv2.medianBlur(gray, 5), 255,
                                  cv2.ADAPTIVE_THRESH_MEAN_C,
                                  cv2.THRESH_BINARY, 9, 10)
    color = cv2.bilateralFilter(image, 9, 300, 300)
    return cv2.bitwise_and(color, color, mask=edges)

# 처리 코드
image = capture_image()
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
results = face_mesh.process(image_rgb)

if results.multi_face_landmarks:
    landmarks = results.multi_face_landmarks[0].landmark

    # 왼쪽 눈 과장
    left_eye_coords = get_landmark_coords(landmarks, LEFT_EYE_INDEXES, image.shape)
    image = warp_region(image, left_eye_coords, scale=1.9)

    # 오른쪽 눈 과장
    right_eye_coords = get_landmark_coords(landmarks, RIGHT_EYE_INDEXES, image.shape)
    image = warp_region(image, right_eye_coords, scale=1.2)

    # 코 과장
    nose_coords = get_landmark_coords(landmarks, NOSE_INDEXES, image.shape)
    image = warp_region(image, nose_coords, scale=1.2)

    # 입 과장
    mouth_coords = get_landmark_coords(landmarks, MOUTH_INDEXES, image.shape)
    image = warp_region(image, mouth_coords, scale=1.5)

    # 만화 효과 적용
    cartooned = cartoon_effect(image)

    # 출력
    plt.figure(figsize=(8, 8))
    plt.imshow(cv2.cvtColor(cartooned, cv2.COLOR_BGR2RGB))
    plt.axis('off')
    plt.title("affine + cartoon")
    plt.show()
else:
    print("얼굴이 감지되지 않았습니다.")

